{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax\n",
    "\n",
    "def softmax(logits):\n",
    "    return jnp.exp(logits - logsumexp(logits, axis=1, keepdims=True))\n",
    "\n",
    "def cross_entropy_loss(weights, inputs, targets):\n",
    "    logits = inputs @ weights\n",
    "    return -jnp.mean(jnp.sum(targets * jnp.log(softmax(logits)), axis=1))\n",
    "\n",
    "def predict(weights, inputs):\n",
    "    logits = inputs @ weights\n",
    "    return softmax(logits)\n",
    "\n",
    "def accuracy(weights, inputs, labels):\n",
    "    preds = predict(weights, inputs)\n",
    "    return jnp.mean(jnp.argmax(preds, axis=1) == jnp.argmax(labels, axis=1))\n",
    "\n",
    "def jax_logistic_regression(X, y, num_epochs=100, lr=0.1):\n",
    "    # Convert labels to one-hot encoding\n",
    "    num_classes = len(jnp.unique(y))\n",
    "    y_onehot = jax.nn.one_hot(y, num_classes)\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = jnp.zeros((X.shape[1], num_classes))\n",
    "\n",
    "    # Gradient function\n",
    "    grad_loss = jit(grad(cross_entropy_loss))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        gradients = grad_loss(weights, X, y_onehot)\n",
    "        weights -= lr * gradients\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Accuracy: {accuracy(weights, X, y_onehot)}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Example usage:\n",
    "# Assume X_train is your input matrix with shape [n_samples, n_features]\n",
    "# and y_train are your labels with shape [n_samples]\n",
    "trained_weights = jax_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(X_train.shape[1:])),\n",
    "        # normalizer,\n",
    "        layers.LSTM(4, activation=\"softmax\"),\n",
    "        # layers.Dropout(0.2),\n",
    "        # layers.Dense(24, activation='relu'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=keras.optimizers.Adam(0.0001),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size=X_train.shape[0])\n",
    "\n",
    "ypred = model.predict(X_test)\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predicted_class_labels = np.argmax(ypred, axis=1)\n",
    "print(\"accuracy score: \", accuracy_score(y_test, predicted_class_labels))\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit,grad,vmap,device_put,random\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "class JaxReg:\n",
    "    \"\"\"\n",
    "    Logistic regression classifier with GPU acceleration support through Google's JAX. The point of this class is fitting speed: I want this\n",
    "    to fit a model for very large datasets (k49 in particular) as quickly as possible!\n",
    "\n",
    "    - jit compilation utilized in sigma and loss methods (strongest in sigma due to matrix mult.). We need to 'partial' the\n",
    "      jit function because it is used within a class.\n",
    "\n",
    "    - jax.numpy (jnp) operations are JAX implementations of numpy functions.\n",
    "\n",
    "    - jax.grad used as the gradient function. Returns gradient with respect to first parameter.\n",
    "\n",
    "    - jax.vmap is used to 'vectorize' the jax.grad function. Used to compute gradient of batch elements at once, in parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=.001, num_epochs=50, size_batch=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.size_batch = size_batch\n",
    "\n",
    "    def fit(self, data, y):\n",
    "        self.K = max(y) + 1\n",
    "        ones = jnp.ones((data.shape[0], 1))\n",
    "        X = jnp.concatenate((ones, data), axis=1)\n",
    "        W = jnp.zeros((jnp.shape(X)[1], max(y) + 1))\n",
    "\n",
    "        self.coeff = self.mb_gd(W, X, y)\n",
    "\n",
    "    # New mini-batch gradient descent function (because jitted functions require arrays which do not change shape)\n",
    "    def mb_gd(self, W, X, y):\n",
    "        num_epochs = self.num_epochs\n",
    "        size_batch = self.size_batch\n",
    "        eta = self.learning_rate\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Define the gradient function using jit, vmap, and the jax's own gradient function, grad.\n",
    "        # vmap is especially useful for mini-batch GD since we compute all gradients of the batch at once, in parallel.\n",
    "        # Special paramaters in_axes,out_axes define the axis of the input paramters (W, X, y) and output (gradients of batches)\n",
    "        # upon which to vectorize. grads_b = loss_grad(W, X_batch, y_batch) has shape (batch_size, p+1, k) for p variables and k classes.\n",
    "\n",
    "        loss_grad = jit(vmap(grad(self.loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            shuffle_index = random.permutation(random.PRNGKey(e), N)\n",
    "            for m in range(0, N, size_batch):\n",
    "                i = shuffle_index[m:m + size_batch]\n",
    "\n",
    "                grads_b = loss_grad(W, X[i, :], y[i])  # 3D jax array of size (batch_size, p+1, k): gradients for each batch element\n",
    "\n",
    "                W -= eta * jnp.mean(grads_b, axis=0)  # Update W with average over each batch\n",
    "        return W\n",
    "\n",
    "    def predict(self, data):\n",
    "        ones = jnp.ones((data.shape[0], 1))\n",
    "        X = jnp.concatenate((ones, data), axis=1)  # Augment to account for intercept\n",
    "        W = self.coeff\n",
    "        y_pred = jnp.argmax(self.sigma(X, W),\n",
    "                            axis=1)  # Predicted class is largest probability returned by softmax array\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, data, y_true):\n",
    "        ones = jnp.ones((data.shape[0], 1))\n",
    "        X = jnp.concatenate((ones, data), axis=1)\n",
    "        y_pred = self.predict(data)\n",
    "        acc = jnp.mean(y_pred == y_true)\n",
    "        return acc\n",
    "\n",
    "    # jitting 'sigma' is the biggest speed-up compared to the original implementation\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def sigma(self, X, W):\n",
    "        if X.ndim == 1:\n",
    "            X = jnp.reshape(X, (-1, X.shape[0]))  # jax.grad seems to necessitate a reshape: X -> (1,p+1)\n",
    "        s = jnp.exp(jnp.matmul(X, W))\n",
    "        total = jnp.sum(s, axis=1).reshape(-1, 1)\n",
    "        return s / total\n",
    "\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def loss(self, W, X, y):\n",
    "        f_value = self.sigma(X, W)\n",
    "        loss_vector = jnp.zeros(X.shape[0])\n",
    "        for k in range(self.K):\n",
    "            loss_vector += jnp.log(f_value + 1e-10)[:, k] * (y == k)\n",
    "        return -jnp.mean(loss_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
